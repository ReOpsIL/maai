import os
import google.generativeai as genai
from google.api_core import exceptions as google_exceptions
import re # Keep re for potential future parsing if needed
from .base_agent import BaseAgent

class ReviewerAgent(BaseAgent):
    """
    Reviews the generated Python code using an LLM for quality, correctness,
    adherence to specifications (impl.md), and best practices.
    """

    def run(self, generated_files: list[str] | None = None, impl_content: str | None = None) -> str | None:
        """
        Executes the Reviewer agent's task using an LLM. Reads generated code,
        compares against the implementation plan, and writes feedback to
        'docs/review.md' if issues are found.

        Args:
            generated_files: A list of absolute paths to the Python files
                             generated by the Coder Agent. If None, reads all .py files
                             from the project's src directory.
            impl_content: The content of the implementation plan (impl.md). If None,
                          it attempts to read 'docs/impl.md'.

        Returns:
            The absolute path to the 'docs/review.md' file if feedback was generated,
            otherwise None.
        """
        self.logger.info(f"Running AI Reviewer Agent for project: {self.project_name}")
        # --- Input Validation and Preparation ---
        if impl_content is None:
            impl_md_path = os.path.join(self.docs_path, "impl.md")
            self.logger.info(f"Implementation plan content not provided, attempting to read from: {impl_md_path}")
            impl_content = self._read_file(impl_md_path)
            if not impl_content:
                self.logger.error(f"Cannot perform review: Implementation plan '{impl_md_path}' not found or empty.")
                raise FileNotFoundError(f"Implementation plan '{impl_md_path}' is required for review.")

        if generated_files is None:
            self.logger.info("Generated files list not provided, scanning project 'src' directory...")
            src_path = os.path.join(self.project_path, 'src')
            generated_files = []
            if os.path.isdir(src_path):
                for root, _, files in os.walk(src_path):
                    for file in files:
                        if file.endswith('.py'):
                            generated_files.append(os.path.join(root, file))
            if not generated_files:
                 self.logger.warning("No Python files found in 'src' directory. Nothing to review.")
                 # Ensure no stale review file exists
                 review_md_path = os.path.join(self.docs_path, "review.md")
                 self._remove_file_if_exists(review_md_path)
                 return None # Nothing to review

        self.logger.info(f"Reviewing {len(generated_files)} file(s).")

        # --- Read Source Code ---
        source_code_content = {}
        self.logger.info("Reading generated source code for review...")
        for file_path in generated_files:
             relative_path = os.path.relpath(file_path, self.project_path) # Path relative to project root
             content = self._read_file(file_path)
             if content:
                 source_code_content[relative_path] = content
             else:
                 self.logger.warning(f"Could not read source file for review: {file_path}")

        if not source_code_content:
             self.logger.error("Could not read any source code content. Aborting review.")
             self.logger.error("Could not read any source code content. Aborting review.")
             # Ensure no stale review file exists
             review_md_path = os.path.join(self.docs_path, "review.md")
             self._remove_file_if_exists(review_md_path)
             raise IOError("Review failed: Could not read source code files.")
 
         # --- Perform AI Review ---
        self.logger.info("Attempting to perform code review using AI...")
        feedback = None
        review_md_path = os.path.join(self.docs_path, "review.md")
        try:
             feedback = self._perform_ai_review(impl_content, source_code_content)
 
             if feedback:
                 self.logger.warning("AI Review found issues. Writing feedback...")
                 self.logger.debug(f"AI Feedback:\n{feedback}")
                 self._write_file(review_md_path, feedback)
                 self.logger.info(f"Review feedback saved to: {review_md_path}")
                 return review_md_path # Return path to feedback file
             else:
                 self.logger.info("AI Review passed. No significant issues found.")
                 # Remove any previous review file if review now passes
                 self._remove_file_if_exists(review_md_path)
                 return None # Indicate success with no feedback file generated
 
        except (ValueError, ConnectionError, RuntimeError, FileNotFoundError, IOError) as e:
             self.logger.error(f"AI Review failed: {e}")
             # Remove potentially incomplete review file on error
             self._remove_file_if_exists(review_md_path)
             raise # Re-raise the exception to be caught by the orchestrator
        except Exception as e:
             self.logger.error(f"An unexpected error occurred during AI review: {e}", exc_info=True)
             self._remove_file_if_exists(review_md_path)
             raise RuntimeError(f"An unexpected error occurred during AI review: {e}")


    def _perform_ai_review(self, impl_content: str, source_code: dict[str, str]) -> str | None:
        """Uses the LLM to review the code against the implementation plan and best practices."""
        if not self.model:
            self.logger.error("Generative model not initialized in BaseAgent. Cannot proceed.")
            raise RuntimeError("ReviewerAgent requires a configured Generative Model.")

        prompt = self._create_review_prompt(impl_content, source_code)
        self.logger.debug(f"Generated review prompt for Gemini:\n{prompt[:500]}...")

        try:
            self.logger.info("Sending request to Gemini API for code review...")
            # Consider safety settings if reviewing potentially complex/unsafe code
            # safety_settings = {...}
            # response = self.model.generate_content(prompt, safety_settings=safety_settings)
            response = self.model.generate_content(prompt)
            review_text = response.text.strip()
            self.logger.info("Received review response from Gemini API.")
            self.logger.debug(f"Raw AI Review Response:\n{review_text}")

            # --- Interpret LLM Response ---
            # Simple interpretation: Look for keywords indicating success or failure/feedback.
            # A more robust approach might involve asking the LLM for structured output (e.g., JSON).
            review_text_lower = review_text.lower()

            # Keywords indicating successful review (adjust as needed based on prompt tuning)
            success_keywords = ["code looks good", "no major issues found", "review passed", "looks correct", "adheres to the plan"]
            # Keywords indicating feedback is present
            feedback_keywords = ["issues found", "needs changes", "recommendations:", "feedback:", "consider changing", "does not adhere"]

            review_passed = any(keyword in review_text_lower for keyword in success_keywords)
            has_feedback = any(keyword in review_text_lower for keyword in feedback_keywords)

            # If success keywords are present AND no strong feedback keywords, assume pass.
            # If feedback keywords are present, assume fail and return the text.
            # If neither, lean towards failure/requesting clarification (or refine prompt).
            if review_passed and not has_feedback:
                return True, None
            elif has_feedback:
                 # Format feedback slightly for clarity
                 formatted_feedback = f"AI Review Feedback:\n-------------------\n{review_text}"
                 return False, formatted_feedback
            else:
                 # Ambiguous response - treat as failure for safety
                 self.logger.warning(f"AI review response was ambiguous: '{review_text[:100]}...'. Treating as failure.")
                 formatted_feedback = f"AI Review Feedback (Ambiguous - Needs Attention):\n-------------------\n{review_text}"
                 return formatted_feedback

        except google_exceptions.GoogleAPIError as e:
            self.logger.error(f"Gemini API Error (Reviewer): {e}", exc_info=True)
            raise ConnectionError(f"Gemini API request failed for code review: {e}")
        except Exception as e:
            self.logger.error(f"An unexpected error occurred during Gemini API call (Reviewer): {e}", exc_info=True)
            raise RuntimeError(f"Failed to perform AI code review: {e}")


    def _create_review_prompt(self, impl_content: str, source_code: dict[str, str]) -> str:
        """Creates the prompt for the LLM to perform code review."""

        source_code_blocks = []
        for path, code in source_code.items():
             # Include filename relative to project root (e.g., src/module.py)
             source_code_blocks.append(f"```python filename={path}\n{code}\n```")
        source_code_section = "\n\n".join(source_code_blocks) if source_code_blocks else "*(No source code provided)*"

        prompt = f"""
Act as an expert Python code reviewer. Analyze the following Python source code generated based on the provided implementation plan.

**Implementation Plan (impl.md):**
```markdown
{impl_content}
```

**Generated Source Code:**
{source_code_section}

**Review Task:**

1.  **Adherence to Plan:** Does the code accurately implement the components, functions, classes, logic, and data structures specified in the implementation plan? Note any significant deviations or missing pieces.
2.  **Correctness:** Does the code appear logically correct for its intended purpose based on the plan? Identify potential bugs, edge cases missed, or incorrect logic.
3.  **Python Best Practices:** Does the code follow standard Python conventions (PEP 8 where applicable)? Is it readable, well-structured, and maintainable? Check for appropriate use of language features, error handling, and resource management.
4.  **Clarity & Simplicity:** Is the code unnecessarily complex? Can any parts be simplified or made clearer?
5.  **Security (Basic Check):** Are there any obvious security vulnerabilities (e.g., hardcoded secrets, potential injection points - if applicable based on code)?

**Output Format:**

*   **If the code looks good** and generally adheres to the plan with no major issues, respond with a brief confirmation like "Code looks good. Adheres to the plan with minor/no issues."
*   **If significant issues are found,** provide clear, concise, and actionable feedback. Structure the feedback point-by-point, referencing the specific file (`filename=...`) and line numbers where possible. Explain *why* something is an issue and suggest *how* it could be improved or corrected. Focus on feedback that the Coder Agent can use to revise the code.

**Example Feedback Output:**

```
AI Review Feedback:
-------------------
Issues found:
1.  **File:** `src/utils.py` (Line 15): The error handling for file reading is incomplete; it should catch specific exceptions like `IOError`.
2.  **File:** `src/main.py` (Function `process_data`): The logic does not seem to handle the edge case where the input list is empty, as specified in the plan (Section 4.2). Add a check for an empty list.
3.  **General:** Consider using f-strings for string formatting in `src/api_client.py` for better readability (PEP 498).
```

**Provide your review below:**
"""
        return prompt

    def _remove_file_if_exists(self, file_path: str):
        """Removes a file if it exists, logging outcome."""
        if os.path.exists(file_path):
            try:
                os.remove(file_path)
                self.logger.info(f"Removed existing file: {file_path}")
            except OSError as e:
                self.logger.warning(f"Could not remove existing file '{file_path}': {e}")